{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note on matrix inversion\n",
    "\n",
    "## Inverse\n",
    "\n",
    "1. The inverse exists if and only if elimination produces n pivots (row exchanges allowed)\n",
    "\n",
    "2. The matrix A cannot have two different inverses. Suppose $ BA = I $ and also $ AC = I $. Then $ B = C $, according to this \"proof by parentheses\": $ B(AC) = (BA)C $ gives $ BI = IC $ which is $ B = C $\n",
    "\n",
    "3. If A is invertible, the one and only solution to $Ax = b$ is $ x = A^{-1}b$\n",
    "4. **Suppose there is a nonzero vector $x$ such that** $Ax = 0$. Then A _cannot have an inverse_ \n",
    "5. A 2 by 2 matrix is invertible if and only if $ad-bc$ is not zero:\n",
    "\n",
    "$$\n",
    "{\\begin{bmatrix}\n",
    "a & b\\\\\n",
    "c & d\n",
    "\\end{bmatrix}}^{-1} = \\frac{1}{ad-bc}\n",
    "\\begin{bmatrix}\n",
    "d & -b \\\\\n",
    "-c & a\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "6. A diagonal matrix has an inverse provided no diagonal entries are zero\n",
    "\n",
    "If $ A = \\begin{bmatrix} d_1 & \\\\ & \\ddots & \\\\ & & d_n \\end{bmatrix} $ then $ A^{-1} = \\begin{bmatrix} 1/d_1 & \\\\ & \\ddots & \\\\ & & 1/d_n \\end{bmatrix} $\n",
    "\n",
    "## The Inverses come in reverse order\n",
    "\n",
    "1. Inverse of $AB$ $(AB)^{-1} = B^{-1}A^{-1}$\n",
    "2. Invertible = Nonsingular (n pivots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The transpose Matrix\n",
    "\n",
    "## Transpose\n",
    "1. The **transpose** of $A$ is denoted by $A^T$\n",
    "2. Entries of $A^T (A^T_{ij} = A_{ji}) $\n",
    "3. The transpose of $AB$ is $(AB)^T = B^TA^T $\n",
    "4. The transpose of $A^{-1}$ is $(A^{-1})^T = (A^T)^{-1}$\n",
    "\n",
    "## Symmetric Metrix\n",
    "\n",
    "**A symmetric matrix is a matrix that equals its own transpose: $A^T = A$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some calculation example using ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-3.        ,  1.33333333],\n",
       "       [ 1.        , -0.33333333]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pylab inline\n",
    "a = array([[1,4],[3,9]])\n",
    "linalg.inv(a) # inverse of a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4. -1. -1. -1.]\n",
      " [-1.  4. -1. -1.]\n",
      " [-1. -1.  4. -1.]\n",
      " [-1. -1. -1.  4.]]\n",
      "[[ 0.4  0.2  0.2  0.2]\n",
      " [ 0.2  0.4  0.2  0.2]\n",
      " [ 0.2  0.2  0.4  0.2]\n",
      " [ 0.2  0.2  0.2  0.4]]\n"
     ]
    }
   ],
   "source": [
    "a = 5 * eye(4) - ones((4,4))\n",
    "print(a)\n",
    "print(linalg.inv(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special matrices and Applications\n",
    "\n",
    "Large matrices almost always have a clear pattern - frequently a pattern of symmetry, and _very many zero entries_.\n",
    "\n",
    "Since a sparce matrix contains far fewer than $n^2$ pieces of information, the computations ought to be fast. We look at _band matrices_ to see how concentration near the diagonal speeds up elimination.\n",
    "\n",
    "**Matrix itself can be seen in equation**\n",
    "The continuous problem asks for $u(x)$ at every $x$, and a computer cannot solve it exactly. It has to be approximated by a discrete problem -- the more unknowns we keep, the better will be the accuracy and the greater the expense. Our choise falls on the differential equation:\n",
    "\n",
    "$$ -\\frac{d^2u}{dx^2} = f(x), 0 \\leq x \\leq 1 $$\n",
    "\n",
    "This is a linear equation for the unknown function $u(x)$. Any combination $C + Dx$ could be added to any solution, since the second derivative of $C + Dx$ contributes nothing. The uncertainty left by these two arbitrary constants C and D is removed by a _\"boundary condition\"_ at each end of the interval:\n",
    "\n",
    "$$ u(0) = 0, u(1) = 0 $$\n",
    "\n",
    "The result is a _two-point boundary-value problem_\n",
    "\n",
    "We accept a finite amount of information about $f(x)$, say its values at $n$ equally spaced points $x = h, x = 2h, \\dots, x = nh$. We compute approximate values $u_1,\\dots,u_n$ for the true solution $u$ at these same points. At the ends $x = 0$ and $x = 1 = (n+1)/h$, the boundary values are $u_0 = 0$ and $u_{n+1} = 0$.\n",
    "\n",
    "The first question: how do we replace the derivative $d^2u/dx^2$? The first derivative can be approximated by stopping at $\\Delta u/\\Delta x$ at a finite stepsize, and not permitting $h$ (or $\\Delta x$) to approach zero. The different $\\Delta u$ can be _forward_, _backward_, or _centered_:\n",
    "\n",
    "\\begin{equation} \\Delta u/\\Delta x = \\frac{u(x+h) - u(x)}{h} \\end{equation} or \\begin{equation} \\frac{u(x) - u(x-h)}{h} \\end{equation} or \\begin{equation} \\frac{u(x+h) - u(x-h)}{2h} \\end{equation}\n",
    "\n",
    "The last is symmetric about x and it is the most accurate. For the second derivative there is just one combination that uses only the values at $x$ adn $x\\pm h$\n",
    "\n",
    "**Second difference** $$ \\frac{d^2u}{dx^2} \\approx \\frac{\\Delta^2u}{\\Delta x^2} = \\frac{u(x+h) - 2u(x) + u(x-h)}{h^2} $$\n",
    "\n",
    "This has the merit of being symmetric about $x$.\n",
    "\n",
    "**Difference equation** $ -u_{j+1} + 2u_j - u_{j-1} = h^2f(jh) $ for $ j = 1,\\dots,n$\n",
    "\n",
    "The structure of these $n$ equations can be better visualized in matrix form. We choose $h = \\frac{1}{6}$, to get a 5 by 5 matrix A:\n",
    "\n",
    "**Matrix Equation** \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2 & -1 & & & \\\\\n",
    "-1 & 2 & -1 & & \\\\\n",
    "& -1 & 2 & -1 & \\\\\n",
    "& & -1 & 2 & -1 \\\\\n",
    "& & & -1 & 2 \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\\\ u_4 \\\\ u_5 \\end{bmatrix} = h^2\n",
    "\\begin{bmatrix}\n",
    "f(h) \\\\\n",
    "f(2h) \\\\\n",
    "f(3h) \\\\\n",
    "f(4h) \\\\\n",
    "f(5h) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The matrix possesses many special properties, and three of those properties are fundamental:\n",
    "\n",
    "1. **The matrix A is tridiagonal**\n",
    "All nonzero entries lie on the main diagonal and the two adjacent diagonals. Outside this band all entries are $a_{ij} = 0$. These zeros will bring a tremendous simplification to Gaussian elimination.\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "2 & -1 & & & \\\\\n",
    "-1 & 2 & -1 & & \\\\\n",
    "& -1 & 2 & -1 & \\\\\n",
    "& & -1 & 2 & -1 \\\\\n",
    "& & & -1 & 2 \\\\\n",
    "\\end{bmatrix} \\rightarrow \\begin{bmatrix}\n",
    "2 & -1 & & & \\\\\n",
    "0 & 3/2 & -1 & & \\\\\n",
    "& -1 & 2 & -1 & \\\\\n",
    "& & -1 & 2 & -1 \\\\\n",
    "& & & -1 & 2 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "2 major simplifications:\n",
    "* There was only one nonzery entry below the pivot\n",
    "* The pivot row was very short.\n",
    "\n",
    "The final result is $LDU = LDL^T$ factorization of A:\n",
    "\n",
    "$$ A = \\begin{bmatrix}1 & & & & \\\\\n",
    "-\\frac{1}{2} & 1 & & & \\\\\n",
    "& -\\frac{2}{3} & 1 & & \\\\\n",
    "& & -\\frac{3}{4} & 1 & \\\\\n",
    "& & & -\\frac{4}{5} & 1 \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "\\frac{2}{1} & & & & \\\\\n",
    "& \\frac{3}{2} & & \\\\\n",
    "& & \\frac{4}{3} & \\\\\n",
    "& & & \\frac{5}{4} \\\\\n",
    "& & & & \\frac{6}{5} \\\\\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "1 & -\\frac{1}{2} & & & \\\\\n",
    "& 1 & -\\frac{2}{3} & \\\\\n",
    "& & 1 & -\\frac{3}{4} \\\\\n",
    "& & & 1 & -\\frac{4}{5} \\\\\n",
    "& & & & 1 & \\\\\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "A **band matrix** has $a_{ij} = 0$ except in the band $|i - j| \\lt w$\n",
    "\n",
    "2. **The matrix is symmetric**\n",
    "Each entry $a_{ij}$ equals its mirror image $a_{ji}$, so that $A^T = A$. The upper triangular $U$ will be the transpose of the lower triangular $L$, and $A = LDL^T$. This symmetry of $A$ reflects the symmetry of $d^2u/dx^2$. An odd derivative like $du/dx$ or $d^3u/dx^3$ would destroy the symmetry.\n",
    "\n",
    "3. **The matrix is positive definite**\n",
    "This extra property says that the _pivots are positive_. Row exchanges are unnecessary in theory and in practice.\n",
    "\n",
    "# Roundoff Error\n",
    "\n",
    "We expect a roundoff error with each matrix operation. Often we keep a fixed number of significant digits. The adding two number of different size gives an error:\n",
    "\n",
    "**Roundoff Error**   $ .456 + .00123 \\rightarrow .457$\n",
    "\n",
    "A small pivot forces a practical change in elimination. Normally we compare each pivot with all possible pivots in the same column. Exchanging rows to obtain the largest possible pivot is called **partial pivoting**.\n",
    "\n",
    "The strategy of _complete pivoting_ looks also in all later columns for the largest possible pivot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Square\n",
    "\n",
    "### Projections and Least Squares\n",
    "\n",
    "It is much better to _choose the x that minimizes an average error E in the m equations_. The most convenient \"average\" comes from the **Squared error**\n",
    "\n",
    "**Least squares solution**: \n",
    "$$\\tilde{x} = \\frac{a^Tb}{a^Ta}$$\n",
    "\n",
    "In general case, we \"solve\" $ax = b$ by minimizing\n",
    "\n",
    "$E^2 = ||ax - b||^2 = (a_1x - b_1)^2 + \\cdots + (a_mx - b_m)^2$\n",
    "\n",
    "The derivative of $E^2$ is zero at the point $\\hat{x}$, if\n",
    "\n",
    "$(a_1\\hat{x}-b_1)a_1 + \\cdots + (a_m\\hat{x}-b_m)a_m = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares Problems with Several Variables\n",
    "\n",
    "**to project b onto a subspace** -- rather than just onto a line. This problem arises from $Ax = b$ when $A$ is an $m$ by $n$ matrix.\n",
    "\n",
    "_Probably, there will not exist a choice of x that perfectly fits the data b_. The problem is to choose $\\hat{x}$ so as to minimize the error, and again this minimization will be done in the least-squares sense. The error is $E = ||Ax - b||$, and **this is exactly the distance from $b$ to the point $Ax$ in the column space**. Searching for the least-squares solution $\\hat{x}$, which minimizes $E$, is the same as locating the point $p = A\\hat{x}$ that is closer to $b$ than any other point in the column space.\n",
    "\n",
    "**The error vector** $e = b - A\\hat{x}$ **must be perpendicular to that space**\n",
    "\n",
    "Finding $\\hat{x}$ and the projection $p = A\\hat{x}$ is so fundamental that we do it in two ways:\n",
    "\n",
    "1. All vectors perpendicular to the column space lie in the left nullspace. Thus the error vector $e = b-A\\hat{x}$ must be in the nullspace of $A^T$:\n",
    "$$A^T(b-A\\hat{x}) = 0$$ or $$A^TA\\hat{x} = A^Tb$$\n",
    "\n",
    "2. The error vector must be perpendicular to each column $a_1,\\cdots,a_n$ of $A$\n",
    "\n",
    "When $Ax = b$ is inconsistent, its least-squares solution minimizes $||Ax - b||^2:\n",
    "\n",
    "**Normal equations** $A^TA\\hat{x} = A^Tb$\n",
    "\n",
    "**Best estimate** $\\hat{x} = (A^TA)^{-1}A^Tb$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection Matrices\n",
    "\n",
    "$P = A(A^TA)^{-1}A^T$\n",
    "\n",
    "The projection matrix has 2 basic properties:\n",
    "* It equals its square $P^2 = P$\n",
    "* It equals its transpose $P^T = P$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Least Squares\n",
    "\n",
    "**Weighted error**: $E^2 = w_1^2(x-b_1)2 + w_2^2(x - b_2)^2$\n",
    "\n",
    "If $w_1 > w_2$, more importance is attached to $b_1$. The minimizing process (derivative = 0) tries harder to make $(x - b_1)^2$ small:\n",
    "\n",
    "$\\frac{dE^2}{dx} = 2\\left[w_1^2(x-b_1) + w_2^2(x-b_2)\\right] = 0$ at $\\hat{x}_w = \\frac{w_1^2b_1 + w_2^2b_2}{w_1^2 + w_2^2}$\n",
    "\n",
    "**The least squares solution to** $WAx = Wb$ is $\\hat{x}_w$:\n",
    "$A^TW^TWA)\\hat{x}_w = A^TW^TWb$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
