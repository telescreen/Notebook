{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Algorithm Implementation\n",
    "\n",
    "The perceptron algorithm is an algorithm for learning a binary classifier called a **threshold function** in neural network where we update $\\theta$ for all $ y^{(i)}(\\theta x^{(i)} + \\theta_0) \\le 0$ \n",
    "\n",
    "$$\n",
    "\\theta = \\theta + y^{(i)}x^{(i)} \\\\\n",
    "\\theta_0 = \\theta_0 + y^{(i)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def perceptron1(x, y, T, debug=False):\n",
    "    error_num = 0\n",
    "    m, n = x.shape    \n",
    "    theta = np.zeros(n)\n",
    "    \n",
    "    for t in range(0, T):\n",
    "        for i in range(0, m):\n",
    "            h = np.inner(theta, x[i])\n",
    "            \n",
    "            # Misclassified points\n",
    "            if y[i] * h <= 0:\n",
    "                error_num += 1\n",
    "                theta = theta + y[i]*x[i]\n",
    "                \n",
    "            if debug:\n",
    "                print('Iter {}: e={}\\th={}\\ty={}\\ttheta={}'\n",
    "                      .format(t*m+i, error_num,\n",
    "                              h, y[i], theta))\n",
    "\n",
    "\n",
    "def perceptron(x, y, T, debug=False):\n",
    "    error_num = 0\n",
    "    m, n = x.shape    \n",
    "    theta = np.zeros(n + 1)\n",
    "    \n",
    "    for t in range(0, T):\n",
    "        for i in range(0, m):\n",
    "            h = np.inner(theta[:-1], x[i]) + theta[-1]\n",
    "            \n",
    "            # Misclassified points\n",
    "            if y[i] * h <= 0:\n",
    "                error_num += 1\n",
    "                theta[:-1] = theta[:-1] + y[i]*x[i]\n",
    "                theta[-1] = theta[-1] + y[i]\n",
    "                \n",
    "            if debug:\n",
    "                print('Iter {}: e={}\\th={}\\ty={}\\ttheta={}'\n",
    "                      .format(t*m+i, error_num,\n",
    "                              h, y[i], theta))                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boundary line does not cross origin ponit.\n",
    "Number of training iteration is defined by variable T\n",
    "\n",
    "#### Problems\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: e=1\th=0.0\ty=1\ttheta=[-1. -1.]\n",
      "Iter 1: e=1\th=-1.0\ty=-1\ttheta=[-1. -1.]\n",
      "Iter 2: e=2\th=-0.5\ty=1\ttheta=[-2.   0.5]\n",
      "Iter 3: e=2\th=1.5\ty=1\ttheta=[-2.   0.5]\n",
      "Iter 4: e=2\th=-2.0\ty=-1\ttheta=[-2.   0.5]\n",
      "Iter 5: e=2\th=2.75\ty=1\ttheta=[-2.   0.5]\n",
      "Iter 6: e=2\th=1.5\ty=1\ttheta=[-2.   0.5]\n",
      "Iter 7: e=2\th=-2.0\ty=-1\ttheta=[-2.   0.5]\n",
      "Iter 8: e=2\th=2.75\ty=1\ttheta=[-2.   0.5]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[-1,-1],[1,0],[-1,1.5]])\n",
    "y = np.array([1,-1,1])\n",
    "perceptron1(x, y, T, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: e=1\th=0.0\ty=-1\ttheta=[-1.  0.]\n",
      "Iter 1: e=1\th=1.0\ty=1\ttheta=[-1.  0.]\n",
      "Iter 2: e=1\th=1.0\ty=1\ttheta=[-1.  0.]\n",
      "Iter 3: e=1\th=-1.0\ty=-1\ttheta=[-1.  0.]\n",
      "Iter 4: e=1\th=1.0\ty=1\ttheta=[-1.  0.]\n",
      "Iter 5: e=1\th=1.0\ty=1\ttheta=[-1.  0.]\n",
      "Iter 6: e=1\th=-1.0\ty=-1\ttheta=[-1.  0.]\n",
      "Iter 7: e=1\th=1.0\ty=1\ttheta=[-1.  0.]\n",
      "Iter 8: e=1\th=1.0\ty=1\ttheta=[-1.  0.]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,0],[-1,1.5],[-1,-1]])\n",
    "y = np.array([-1,1,1])\n",
    "perceptron1(x, y, T, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boundary line cross origin point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: e=1\th=0.0\ty=1\ttheta=[-4.  2.  1.]\n",
      "Iter 1: e=1\th=11.0\ty=1\ttheta=[-4.  2.  1.]\n",
      "Iter 2: e=2\th=3.0\ty=-1\ttheta=[-3.  3.  0.]\n",
      "Iter 3: e=3\th=0.0\ty=-1\ttheta=[-5.  1. -1.]\n",
      "Iter 4: e=3\th=-8.0\ty=-1\ttheta=[-5.  1. -1.]\n",
      "Iter 5: e=3\th=21.0\ty=1\ttheta=[-5.  1. -1.]\n",
      "Iter 6: e=3\th=10.0\ty=1\ttheta=[-5.  1. -1.]\n",
      "Iter 7: e=4\th=3.0\ty=-1\ttheta=[-4.  2. -2.]\n",
      "Iter 8: e=4\th=-6.0\ty=-1\ttheta=[-4.  2. -2.]\n",
      "Iter 9: e=4\th=-10.0\ty=-1\ttheta=[-4.  2. -2.]\n",
      "Iter 10: e=4\th=18.0\ty=1\ttheta=[-4.  2. -2.]\n",
      "Iter 11: e=4\th=8.0\ty=1\ttheta=[-4.  2. -2.]\n",
      "Iter 12: e=5\th=0.0\ty=-1\ttheta=[-3.  3. -3.]\n",
      "Iter 13: e=5\th=-3.0\ty=-1\ttheta=[-3.  3. -3.]\n",
      "Iter 14: e=5\th=-12.0\ty=-1\ttheta=[-3.  3. -3.]\n",
      "Iter 15: e=5\th=15.0\ty=1\ttheta=[-3.  3. -3.]\n",
      "Iter 16: e=5\th=6.0\ty=1\ttheta=[-3.  3. -3.]\n",
      "Iter 17: e=5\th=-3.0\ty=-1\ttheta=[-3.  3. -3.]\n",
      "Iter 18: e=5\th=-3.0\ty=-1\ttheta=[-3.  3. -3.]\n",
      "Iter 19: e=5\th=-12.0\ty=-1\ttheta=[-3.  3. -3.]\n",
      "Iter 20: e=5\th=15.0\ty=1\ttheta=[-3.  3. -3.]\n",
      "Iter 21: e=5\th=6.0\ty=1\ttheta=[-3.  3. -3.]\n",
      "Iter 22: e=5\th=-3.0\ty=-1\ttheta=[-3.  3. -3.]\n",
      "Iter 23: e=5\th=-3.0\ty=-1\ttheta=[-3.  3. -3.]\n",
      "Iter 24: e=5\th=-12.0\ty=-1\ttheta=[-3.  3. -3.]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[-4,2],[-2,1],[-1,-1],[2,2],[1,-2]])\n",
    "y = np.array([1,1,-1,-1,-1])\n",
    "perceptron(x,y,5,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning as Optimization problem\n",
    "\n",
    "Given we have boundary line describe by $\\theta x + \\theta_0 = 0$. We could push the boundary line to positive/negative side by drawing parallel line with boundary line, with distance from boundary line is 1. These 2 line are called marginal boundary.\n",
    "\n",
    "$$ \\theta x + \\theta_0 = \\pm 1 \\ $$\n",
    "\n",
    "We want these 2 margin boundaries stay as far from as decision boundary as possible. We call a parameter to regular this distance: regularization term.\n",
    "\n",
    "We don't want our margin boundaries increase the number of point misclassified. We want to minize the error. \n",
    "\n",
    "Our objective function now is \n",
    "\n",
    "**objective = error + regularization**\n",
    "\n",
    "After fixing the $\\theta$ and $\\theta_0$, our decision boundary is defined as $\\theta x + \\theta_0 = 0$. We observer that when we move away from decision boundary, the distance between margin boundaries and decision boundary increase at the rate related to the magnitude of $\\theta$. \n",
    "\n",
    "we define the distance from a misclassified point to the decision boundary as\n",
    "\n",
    "$$\n",
    "\\gamma(\\theta, \\theta_0) = \\frac{y^{(i)}(\\theta x^{(i)} + \\theta_0)}{||\\theta||}\n",
    "$$\n",
    "\n",
    "the $y^{(i)}(\\theta x^{(i)} + \\theta_0)$ measures the classified result with the real label. If the value is greater than 0, we know that the point lays in the correct side from decision boundary. This value is called **agreement**.\n",
    "\n",
    "So we have\n",
    "\n",
    "* Hinge loss $\\text{Loss } h(y^{(i)}(\\theta x^{(i)} + \\theta_0))$\n",
    "* Regularization: to maximize margin $ \\text{max } \\frac{1}{||\\theta||}$ or $ \\text{min } \\frac{1}{2}||\\theta||^2$\n",
    "* The objective function: \n",
    "$$ J(\\theta, \\theta_0) = \\frac{1}{n}\\sum_{i=1}^n\\text{Loss}(y^{(i)}(\\theta x^{(i)} + \\theta_0)) + \\frac{\\lambda}{2}||\\theta||^2$$\n",
    "\n",
    "### Optimization algorithms\n",
    "\n",
    "Optimization algorithms\n",
    "* preface: gradient descent optimization\n",
    "* stochastic gradient descent\n",
    "* quadratic program\n",
    "\n",
    "#### Gradient Descent: Geometrically Revisited\n",
    "\n",
    "Assume $\\theta \\in \\mathbb{R}$. Our goal is to find $\\theta$ that minimizes\n",
    "\n",
    "$$ J(\\theta, \\theta_0) = \\frac{1}{n}\\sum_{i=1}^n\\text{Loss}(y^{(i)}(\\theta x^{(i)} + \\theta_0)) + \\frac{\\lambda}{2}||\\theta||^2$$\n",
    "\n",
    "Through gradient descent. In other words, we will\n",
    "\n",
    "1. Start $\\theta$ at an arbitrary location: $\\theta \\leftarrow \\theta_{\\text{start}}$\n",
    "2. Update $\\theta$ repeatedly with $\\theta \\leftarrow \\theta - \\eta \\frac{\\partial J(\\theta, \\theta_0)}{\\partial \\theta}$\n",
    "\n",
    "![image.png](https://prod-edxapp.edx-cdn.org/assets/courseware/v1/21c8b2bc0924ffd746e844c9fc38c980/asset-v1:MITx+6.86x+1T2019+type@asset+block/images_lec4_pic3.svg)\n",
    "\n",
    "\n",
    "#### Stochastic gradient descent (SGD)\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{n}\\sum_{i=1}^n \\Big[\\text{Loss}(y^{(i)}(\\theta x^{(i)} + \\theta_0)) + \\frac{\\lambda}{2}||\\theta||^2\\Big]$$\n",
    "\n",
    "Select $i \\in {1,\\cdots,n}$ at random\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\eta_t \\nabla_{\\theta}\\Big[\\text{Loss}(y^{(i)}\\theta x^{(i)})+ \\frac{\\lambda}{2}||\\theta||^2 \\Big] $$\n",
    "\n",
    "#### Support Vector Machine\n",
    "\n",
    "Support Vector Machine finds the maximum margin linear seperator by solving the quadratic program that corresponds to $J(\\theta, \\theta_0)$\n",
    "We disallow any margin violations, the quadratic program we have to solve is\n",
    "\n",
    "Find $\\theta, \\theta_0$ that minimize $\\frac{1}{2}||\\theta||^2$ subjec to $y^{(i)}(\\theta \\cdot x^{(i)} + \\theta_0) \\ge 1 \\text{,  } i = 1,\\ldots,n$\n",
    "\n",
    "#### Summary\n",
    "\n",
    "* Learning problems can be formulated as optimization problem of the form: loss + regularization\n",
    "* Linear, large margin classification, along with many other learning problems, can be solved with stochastic gradient descent algorithms\n",
    "* Large margin linear classification be also obtained via solving a quadratic program (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: e=1\th=0.0\ty=1\ttheta=[0. 1. 0.]\n",
      "Iter 1: e=2\th=0.0\ty=1\ttheta=[-1.  1.  0.]\n",
      "Iter 2: e=3\th=0.0\ty=1\ttheta=[-1.  1. -1.]\n",
      "Iter 3: e=3\th=1.0\ty=1\ttheta=[-1.  1. -1.]\n",
      "Iter 4: e=3\th=1.0\ty=1\ttheta=[-1.  1. -1.]\n",
      "Iter 5: e=3\th=1.0\ty=1\ttheta=[-1.  1. -1.]\n",
      "Iter 6: e=3\th=1.0\ty=1\ttheta=[-1.  1. -1.]\n",
      "Iter 7: e=3\th=1.0\ty=1\ttheta=[-1.  1. -1.]\n",
      "Iter 8: e=3\th=1.0\ty=1\ttheta=[-1.  1. -1.]\n",
      "Iter 9: e=3\th=1.0\ty=1\ttheta=[-1.  1. -1.]\n",
      "Iter 10: e=3\th=1.0\ty=1\ttheta=[-1.  1. -1.]\n",
      "Iter 11: e=3\th=1.0\ty=1\ttheta=[-1.  1. -1.]\n",
      "Iter 12: e=3\th=1.0\ty=1\ttheta=[-1.  1. -1.]\n",
      "Iter 13: e=3\th=1.0\ty=1\ttheta=[-1.  1. -1.]\n",
      "Iter 14: e=3\th=1.0\ty=1\ttheta=[-1.  1. -1.]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0,1,0],[-1,0,0],[0,0,-1]])\n",
    "y = np.array([1,1,1])\n",
    "perceptron1(x,y,5,True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
