---
title: "Algae-Casestudies"
author: "Habui"
date: "Thursday, October 08, 2015"
output: html_document
---

# Step 0: Prepare for analysis
```{r}
library('DMwR')
library('cluster')
library('rpart')
data(algae)
```


# Step 1: Examine the dataset

The idea is to calculate the missing value based on values of similar rows. Similar could be calculated in several metrics; the most familar, well-known is Euclidean. 


```{r}
summary(algae)
```

A picture worths a thoundsand words. Besides, `summary`, we could plot the dataset to observer its distributions. For example, the plot of `mxPH` could be obtained by:

```{r}
hist(algae$mxPH, prob=T, xlab='', main='Histogram of mxPH', ylim=0:1)
lines(density(algae$mxPH, na.rm=T))
rug(jitter(algae$mxPH))
```

# Step 2: Clean up the data. Process unknown values

## 1. Removing observations with too many unknown values

```{r}
# Remove rows with too many NAs
algae <- algae[-c(62, 199),]
```

## 2. Filling the unknowns with the most frequent values

```{r}
algae[48, 'mxPH'] <- mean(algae$mxPH, na.rm=T)
```

Though, this method is sometime dangerous, and should be done in cautious. For example, the distribution is skewed to higher values, and there are few extreme values that make the mean value highly unrepresentative of the most frequent value. 

```{r}
# Unlikely reasonable
algae[is.na(algae$Chla), 'Chla'] <- mean(algae$Chla, na.rm=T)

```

## 3. Filling the unknowns by exploring correlations

We could find the correlation between variables using function `cor`

```{r}
symnum(cor(algae[,4:18], use='complete.obs'))
```

We exclude three first column because they are normial. The use of `use='complete.obs'` tell R to obmit `NAs` from the calculation of correlation. 

We can observe that the correlations are most of the time irrelevant. However there are 2 exceptions: NH4 and NO3 and oPO4 and PO4. 

We could use linear regression to fill the correlation

```{r}
lm(oPO4 ~ PO4, data=algae)
fillPO4 <- function(o) {
  if (is.na(o)) return(NA)
  return((o+15.6142)/0.6466)
}

algae[is.na(algae$Chla), 'Chla'] <- median(algae$Chla,na.rm=T)
algae[is.na(algae$PO4), 'PO4'] <- sapply(algae[is.na(algae$PO4), 'oPO4'], fillPO4)

```


## 4. Filling the unknowns by exploring similarities between cases.

```{r}
data(algae)
algae <- algae[-c(62, 199),]
central.value <- function(x) {
  if (is.numeric(x)) median(x, na.rm=T)
  else if(is.factor(x)) levels(x)[which.max(table(x))]
  else {
    f <- as.factor(x)
    levels(f)[which.max(table(f))]
  }
}


dist.mtx <- as.matrix(daisy(algae,stand=T))
for(r in which(!complete.cases(algae))) {
  algae[r, which(is.na(algae[r,]))] <- 
    apply(data.frame(algae[c(as.integer(names(sort(dist.mtx[r,])[2:11]))),
                     which(is.na(algae[r,]))]), 
          2, central.value)
}

# Save the clean data at clean.algae
clean.algae <- algae
data(algae)
```


# Step 3: Prediction models

we will explore 2 preditive models that can be applied in algae domain:
* Linear Regression
* Regression Tree

## Multiple linear regression

We could obtain linear regression model using function `lm`

```{r}
lm.a1 <- lm(a1 ~ ., data = clean.algae[,1:12])
lm.a1 <- step(lm.a1)
summary(lm.a1)
```


R handles normial variables by creating a set of auxiliary variables. Namely for each factors with k levels, R create k-1 auxiliary variables. The variables have the values 0 or 1. A value of 1 means the associated value of the factor is "present", and that will mean that other auxiliary variables have value 0. 

**summary** function gives some **diagnostic information** including:
* The residuals of the fit of the linear model and the used data.
* For each coefficients, R will show its value and standard errors, as well as the importance of each coefficient. 
* Proportion of explained variance. 
* F-statistic

We should spend time here explaining a little bit the meaning of "The importance of each coefficient". The importance of a variable represents its influences on the final result. It is measured by "the null hypothesis", which test the valid of the statement: `H0: a variable is null`. In order to accept or reject the hypothesis, we conduct a t-student test.

## Regression Trees

We can obtain regression tree using the following commands:

```{r}
library(rpart)
data(algae)
algae <- algae[-c(62, 199),]
rt.a1 <- rpart(a1 ~ ., data = algae[,1:12])
```

The function `rpart` stop growing the trees when certain criteria are met
* The decrease in the deviance goes below a certain threshold
* The number of samples in the node is less than another threshold
* The tree depth exceeds another value.

The thresholds are controlled by the parameters: *cp*, *minsplit*, *maxdepth*. Their default values are 0.01, 20, and 30 respectively.

we should always check the validity of these default tree growth stopping criteria to prevent overfitting data. This can be carried out through a process of post-pruning of the obtained tree. The rpart package implements a pruning method named _cost complexity_ pruning. This method tries to estimate the value of _cp_ that ensues the best compromise between their predictive accuracy and tree size. 

Given a tree produced by *rpart()*, R can produced a set of sub-trees of this tree and estimate their predictive performance. This information can be obtained using the function *printcp()*.

```{r}
reliable.rpart <- function(form,data, se = 1,cp = 0,verbose=T, ...) {
  tree <- rpart(form, data, cp=cp, ...)
  if (verbose && ncol(tree$cptable) < 5) tree
  else {
    lin.min.err <- which.min(tree$cptable[, 4])
    if (verbose && lin.min.err == nrow(tree$cptable))
      warning("Minimal cross Validation Error is obtained with the largest tree\n. Further tree growth could produce more accurate tree\n")
    tol.err <- tree$cptable[lin.min.err, 4] + se * tree$cptable[lin.min.err, 5]
    se.lin <- which(tree$cptable[,4] <= tol.err)[1]
    prune.rpart(tree, cp=tree$cptable[se.lin, 1]+1e-9)
  }
}

rt.a1 <- reliable.rpart(a1 ~ ., data=algae[,1:12])
```


# Step 4: Model evaluation and Selection

To obtain the prediction of any R model, one uses the function **predict()**. 

```{r}
lm.prediction.a1 <- predict(lm.a1, data=clean.algae)
rt.prediction.a1 <- predict(rt.a1, data=algae)
```

Having a prediction, we can calculate their 

#### Mean Absolute Error:

```{r}
(mae.a1.lm <- mean(abs(lm.prediction.a1 - clean.algae[,'a1'])))
(mae.a1.rt <- mean(abs(rt.prediction.a1 - algae[,'a1'])))
```

#### Mean Squared Error
```{r}
(mse.a1.lm <- mean((lm.prediction.a1 - clean.algae[,'a1'])^2))
(mse.a1.rt <- mean((rt.prediction.a1 - algae[,'a1'])^2))
```

#### Normalized Mean Squared Error

```{r}
(nmse.a1.lm <- mean((lm.prediction.a1 - clean.algae[,'a1'])^2) / mean((mean(clean.algae[,'a1']) - clean.algae[,'a1'])^2))
(nmse.a1.rt <- mean((rt.prediction.a1 - algae[,'a1'])^2) / mean((mean(algae[,'a1']) - algae[,'a1'])^2))
```

The smaller NMSE, the better. 

#### Visual inspection of the prediction

The idea is to plot the whole prediction and obserse their result under the line with angle **pi/4**.

```{r}
old.par <- par(mfrow=c(2,1))
plot(lm.prediction.a1, clean.algae[, 'a1'], main="Linear model", xlab="Prediction", ylab="True Value")
abline(0, 1, lty=2)
plot(rt.prediction.a1, algae[, 'a1'], main="Regression Tree model", xlab="Prediction", ylab="True Value")
abline(0, 1, lty=2)
par(old.par)
```


Several criteria exists for evaluating models. Among the most popular are criteria that calculate the **predictive performance of models**. The predictive performances of models is obtained by comparing the prediction of models with the real values of the target variables. 

The key issue here is to obtain a reliable estimate of a model performance on data for which we do not know the true target value. The measures calculated using the training data are unreliable, because they are biased. In effect, there are models that can easily obtain zero prediction error on the training data, however, this performance will hardly generalize over new samples for which the target variable value is unknown. This phenomenon is usually known as _overfitting_. 

_K-fold Cross Validation_ is among the most frequently used methods of obtaining these reliable estimates for small data sets like our case study. The idea is: otain `K` equally sized and randomized sub-sets of training data. For each of these `K` subsets, build a model using the remaining K-1 sets and evaluate this model on the Kth subset. Store the performance of the model and repeat this process for all remaining subsets. In the end, we have K performance measures, all obtained by testing a model on data not used for its construction. The K-fold Cross Validation estimation is the average of these K measures. A common choice for K is 10.

The following code puts the ideas in practice for 2 models:
* linear regression
* decision tree

```{r, echo=FALSE, results="hide"}
cross.validation <- function(all.data, clean.data, n.folds = 10) {
  n <- nrow(all.data)
  idx <- sample(n, n)
  all.data <- all.data[idx,]
  clean.data <- clean.data[idx,]
  
  n.each.part <- as.integer(n/n.folds)
  
  perf.lm <- vector()
  perf.rt <- vector()
  
  for(i in 1:n.folds) {
    cat('Fold ', i, '\n')
    out.fold <- ((i - 1)*n.each.part+1):(i*n.each.part)
    
    l.model <- lm(a1 ~ ., clean.data[-out.fold, 1:12])
    l.model <- step(l.model)
    l.model.preds <- predict(l.model, clean.data[out.fold, 1:12])
    l.model.preds <- ifelse(l.model.preds < 0, 0, l.model.preds)
    
    r.model <- rpart(a1 ~ ., all.data[-out.fold, 1:12])
    r.model.preds <- predict(r.model, all.data[out.fold, 1:12])
    
    perf.lm[i] <- mean((l.model.preds - all.data[out.fold,1:12])^2) /
      mean((mean(all.data[-out.fold,1:12]) - all.data[out.fold,1:12])^2)
    perf.rt[i] <- mean((r.model.preds - all.data[-out.fold,1:12])^2) /
      mean((mean(all.data[-out.fold,1:12]) - all.data[out.fold,1:12])^2)
  }
  
  list(lm=list(avg=mean(perf.lm), std=sd(perf.lm), fold.res=perf.lm),
    rt=list(avg=mean(perf.rt), std=sd(perf.rt), fold.res=perf.rt))
}

cv10.res <- cross.validation(algae, clean.algae)
```

DMwR inplements a general function to compare multiple alternative models for us. It can be used with different estimation methods:

```{r, results="hide"}
cv.rpart <- function(form,train,test,...) {
  m <- rpartXse(form, train, ...)
  p <- predict(m, test)
  mse <- mean((p-resp(form, test))^2)
  c(nmse=mse/mean((mean(resp(form, test)) - resp(form, test))^2))
}

cv.lm <- function(form,train,test,...) {
  m <- lm(form, train, ...)
  p <- predict(m, test)
  p <- ifelse(p < 0, 0, p)
  mse <- mean((p-resp(form, test))^2)
  c(nmse=mse/mean((mean(resp(form, test)) - resp(form, test))^2))
}

res <- experimentalComparison(c(dataset(a1~., clean.algae[,1:12], 'a1')),
                              c(variants('cv.lm'),
                                variants('cv.rpart', se=c(0,0.5,1))),
                              cvSettings(3,10,1234))
```

We could obtain similar comparative experiment for all 7 prediction tasks

```{r, results="hide"}
DSs <- sapply(names(clean.algae)[12:18], function(x, names.attrs) {
  f <- as.formula(paste(x, "~ ."))
  dataset(f, clean.algae[,c(names.attrs, x)], x)
}, names(clean.algae)[1:11])

res.all <- experimentalComparison(DSs,
                                  c(variants('cv.lm'), 
                                    variants('cv.rpart', se=c(0, 0.5, 1))),
                                  cvSettings(5, 10, 1234))
```

The variability of the results provides good indications that this might be a good candidate for an ensemble approach. Ensembles are model construction methods that fundamentally try to overcome some limitations of individual models by generating a large set of alternative models and then combining their prediction. There are many approaches to obtain ensembles that differ not only in the way the diversity of models is obtained (eg., different training samples, different variables, different modeling techniques, etc.), but also in how the ensemble prediction is reached (eg., voting, averaging, etc.). **Random forest** are regarded as one of the more competitive examples of ensembles.

```{r, results="hide"}
library(randomForest)
cv.rf <- function(form,train,test,...) {
  m <- randomForest(form, train, ...)
  p <- predict(m, test)
  mse <- mean((p-resp(form, test))^2)
  c(nmse=mse/mean((mean(resp(form, train)) - resp(form, test))^2))
}

res.all <- experimentalComparison(DSs,
                                  c(variants('cv.lm'), 
                                    variants('cv.rpart', se=c(0,0.5,1)),
                                    variants('cv.rf', ntree=c(200, 500, 700))),
                                  cvSettings(5, 10, 1234))
```

The model "cv.rf.v2" is the best for algae 1, 2, 4, 6. The following checks the statistical significance of this statement:

```{r}
compAnalysis(res.all, agains='cv.rf.v2', 
             datasets=c('a1', 'a2', 'a4', 'a5', 'a6'))
```

# Prediction for 7 other algae

```{r}
bestModelsNames <- sapply(bestScores(res.all), function(x) x['nmse', 'system'])
learners <- c(rf='randomForest', rpart='rpartXse')
funcs <- learners[sapply(strsplit(bestModelsNames, '\\.'), function(x) x[2])]
parSetts <- lapply(bestModelsNames, function(x) getVariant(x, res.all)@pars)
bestModels <- list()
for (a in 1:7) {
  form <- as.formula(paste(names(clean.algae)[11+a], '~ .'))
  bestModels[[a]] <- do.call(funcs[a], c(list(form, clean.algae[,c(1:11, 11+a)]),
                                         parSetts[[a]]))
}
```

Predict

```{r}
clean.test.algae <- knnImputation(test.algae, k = 10, distData = algae[,1:11])
```

```{r}
preds <- matrix(ncol=7, nrow=140)
for(i in 1:nrow(clean.test.algae)) 
  preds[i,] <- sapply(1:7, 
                      function(x) predict(bestModels[[x]], clean.test.algae[i,]))

avg.preds <- apply(algae[,12:18], 2, mean)
apply(((algae.sols-preds)^2), 2, mean) / apply((scale(algae.sols, avg.preds, F)^2), 2, mean)

```


